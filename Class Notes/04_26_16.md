# COEN 169 Web Infrastructure 04_26_16

## Language Models

    - Roughly two steps
      1) Estimate the probability of the term given the document.
        - P(t|D) = (tft_D/N_D)
      2) Predition
        - P(Q|D) = product of all P(q|D) where q is a term in query Q
        - Rank based on the log(P(Q|D))
          - there will be underflow problems with poducts of many small terms
          - = sum(log(P(q|D))) where q a term in Q

    - Smoothing
      1) Add-one
        - P(t|D) = (tft_D + 1)/(N_n + |V|)
        - There will be mamny zero terms
        - V is vocab size
        - Prior knowledge: treat all terms equally
      2) Dirichlet Smoothing
        - P(t|D) = (tft_D + uP(t|C))/(N_D + u)
          - observations + pseudo-count
          - pseudo-count uses prior knowledge
            - treat all terms based on their frequency in the corpus
          - better prior knowledge gives a better estimate of the probability
        - From bayesian statistics
      3) Linear Smoothing
        - P(t|D) = alpha * P(t|D) + (1-alpha)P(t|C)
          - doc language model, corpus language model
          - will never be 0 -> will always containt the tft term once
          - huge document gets higher weight on observations

## Query Likelihood Retrieval Model

    - two docs with 50 term occurrences
    - same score overall
    - choose the one that has more of the less common term (like ipad in "apple ipad")

END
