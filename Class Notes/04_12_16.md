# COEN 169 Web Infrastructure 04_12_16

## Last Class: Boolean Retrieval

  Exact Match
  Extend to Ranked Boolean

    A AND B
      min(fa, fb)
      OR
      sum/max (fa, fb)

## Vectors

  Length of a vector

    2 Dimensional:
      sqrt(x^2+y^2)
    N Dimensional:
      x=(x1, x2, x3, ... , xn)
      L = sqrt(x1^2 + x2^2 + ... + xn^2)

## Binary Text Representation

## Vector Space Representation

    - Any span o ftext is a vector in V-dimensional space
      - V is the number of terms in the vocabulary
    - Inner product/vector product

      x = (x1,x2)
      y = (y1,y2)
      x * y = x1 * y1 + x2 * y2

      x = (x1,x2,x3)
      y = (y1,y2,y3)
      x * y = x1 * y1 + x2 * y2 + x3 * y3

    - when both components of a dimension are nonzero, you have similarity
      - if one is zero, then that reflects in the sum - no addition of that term
    - Vocabulary overlap between query and document
      - higher value, more relevance

## The Inner Product

    - 1 = the term appears at least once
    - 0 = the term does not appear
    - Use the inner product to rank a document
      - The higher, the more similar
    - Multiply corresponding components and then sum those products
    - Using a binary rep, the inner product corresponds to the number of terms appearing
    - Scoring documents based on their inner-product with the query has a major issue
      - Document length
      - By random chance, a much longer document can have a large overlap
      - Having less tokens, looks more matched
      - Short document vs long document, same number of common terms
        - Short will look more relevant

    d1 50 tokens
    d2 100 tokens
    inner product(q,d1)/(length(q) * length(d1))
    inner product(q,d2)/(length(q) * length(d2))
    cosine sim

## The cosine Similarity

    - Measure the cosine of the angle between the two vectors
    - The numerator is the inner product
    - The denominator "normalizes" for document length
    - Ranges from - to 1 (equals 1 i fthe vectors are identical)
    - Determines if the two vectors are pointing in the same direction

## Excercise

    cosine("dog bite", "man dog") = ?
    {bite man dog}
    d1 = {1, 0, 1}
    d2 = {0, 1, 1}
    (1 * 1 + 0 * 1 + 1 * 0)/(sqrt(1^2+0^2+1^2) * sqrt(1^2_1^2+0^2))
    = 0.5

## Representations

    - Binary
      - Had assumed binary vectors
    - Term Frequency
      - Stop words will throw off the inner product
      - They will have high values, give a high product
    - Term Frequency * Term Weighting

      Wt = 1/Dft
      Df: # of docs containing t
      1/N
      Inverse document frequency

## Inverse Document Frequncy (IDF)

    idf_t = log(N/df_t)
    N = number of documents in the collection
    df_t = number of documents in whihc term t appears

## TF.IDF

    tf_t * idf_t

      - tf_t
        - greater when the term is frequent

## TF.IDF/Caricature Analogy

    - TD.IDF accentuates terms that are frequent in the docuemnt, but not frequent in general
    - Caricature: exaggerates traits that are chracteristic of the person (compared to the average)

## Queries as TF.IDF Vectors

    - Terms tend to appear only once in the query
    - TF usually equals 1
    - IDF is computed using the collection statistics
    - N is the total number of documents in the corpus
    - Terms appearing in fewer documents get a specific weight

## Putting everything together

    - rank document
    - TF: favors terms that are frequent in the document
    - IDF: favors terms taht do not occur in many documents

## Sub Linear TF Scaling

    - use logs, then see how big the difference is
    - 2 million vs 1 million
    - 2000 vs 1000
    - Is it really twice as much?
    - common practice: take a log
    - y = 1 + log...

    (1+log(tf_t_)) * log(N/df_t)

## Vector Space Model

    - Any text can be seen as a vector in V-dimensional space
      - document
      - query
      - sentence
      - word
      - etc
    - Rank documents based on their cosine similarity to query
    - If a document i ssimilar to the query, it i slikely to be relevant

## Vector Space Representations

    - Find me ___ that is similar to ___
    - As long as ___ and ___ are associated with text, one potential solution is
      - represent thes items as tf.idf term-weight vectors

## Multimedia retrieval

    - feature extraction
    - retrieval model

END
