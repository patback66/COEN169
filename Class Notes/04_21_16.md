# COEN 169 Web Infrastructure

## Unigram Language Model

    - Statistical language model - based on statistics
    - bag of words/balls - probability of drawing each
    - Training proces - train a model before seeing the query
        - train to know the probabilities first
        - Use query to rank documents first
        - Ranking score is the conditional probability
            - P(Q|D) = not P(W|D)
            - Query likelihood
            - Independence assumption
            - P(AB) = P(A)P(B)
            - P(AB) = P(A|B)P(B) << if not independendent
            - Real implementation: take the log of the query likelihood
            - so P(Q|D) = sum(log(W|D))

## IMDB Corpus example

    - More probable? "artist of the year" vs "movie of the year"
      - Movie has a higher probability

## Language Models

    - Probability distribution defined over a particular vocabulary

## Topic models

    - measure similarity between the distributions
      - query vs document
    - K-L divergence for measuring the difference between the two

## Document Language Models

    - Topics discussed

## Query-Likelihood Model

    - Each document is scored according to the probability that the doc will be generated by the query
    - But what if one element doesn't exist in all?

## Smoothing Probaility Estimates

    - Artificially enhance so that bag has all >> no zero probability
    - Like add a head and one tail
    - psuedo-count + prior knowledge

    - before Smoothing
      - P(t|D)=tftD/N_d
    - after smoothing
      - P(t|D)=(tftd+1)/(N_d + |V|) > 0
    - tftD "term frequency of t in the document"
    - V is the voc of the whole corpus

## Linear Smoothing

    - P(t|D) = alphaP(t|D) + (1-alpha)P(t|C) > 0
      - P(t|C) = (t_f_t * c)/N_c > 0
    - P(t|D)  = (tft_d + muP(t|C))/(N_D + |V|)
    - P(t|D)  = (tft_d + muP(t|C))/(N_D + mu)
      - mu is a large number, like 2000
      - sum(muP(t|c)) << t in V
        - = mu sum(P(t|c)) << t in V
        - = mu
        - number of terms you add
        - usually 2000 or 5000
    - P(t|D) = (tft_d + muP(t|C))/(N_D + mu)
      - = 1/(N_D + mu) * tft_d + mu/(N_D + mu) * P(t|C)
      - = N_D/(N_D + mu) * tft_d/N_D + mu/(N_D + mu) * P(t|C)
        - tft_d/N_D = p(t|D)
      - looks like linear smoothing
      - adaptive
      - N_D inc, alpha inc
      - make better estimation for the corpus

END
