# COEN 179 04_05_16

  Eyor

## Statistical Properties of Text

    - How is the frequency distributed
    - How fast does vocab size grow witht he corpus

## Word Frequency

    - a few common
    - most rare
    - heavy tailed distribution

## Zipf's Law

    - Rank (r): the numerical position of a word in a list sorted by decreasing frequency f
    - 1949
    - Statistically empirical law, not an exact physical law!

      f x r = k (for constant k)
      frequency x rank
      7.3 x 1 ~= 7.3 million
      3.8 x 2 ~= 7.6 million

      f = k / r
      log(f) = log(k/r)
      log(f) = log(k) - log(r)

    - We should be able to plot log(f) vs log(r) and see a linear relationship
    - straight line with slope = -1
    - If prob of word of rank r is pr and N is total number of word occurences in corpus

      Pr = f/N = A/r for corpus independent const A ~= 0.1
      ~ A/r = 0.1/1 = 0.1 //not very good approximation at low/high ranks

    - Holds for different
      - Language
      - size
      - genres
      - topics
      - complexity of content

## Maneldbrot (1954) Correction

    - More general form for better fit

      f = P(r + rho)^(-B) for constants P, B, rho

## Zipf's Law in Web search queries

    - same trend

## Zipf's Law Impact on IR

    - Good
      - Stopwords will account for a large fraction, so eliminate
    - Bad
      - For most words, sufficient data is difficult since rare

## Heap's Law

    - If V is the size of vocab and the n is length of corpus:
      - V = Kn^Beta with constants K, 0<Beta<1
    - Typical constatns
      - K ~= 10-100
      - Beta ~= 0.4-0.6

## Evaluation

    - Evaluation Criteria
      - Effectiveness
      - Efficiency
      - Usability

## Why system Evaluation

    - Many retreival models, which is best?
    - Best component for?
      - term selection
      - term weighting
        - TF-IDF, TF
      - ranking function
    - How far down the ranked list will a user need to look?
      - to find some/all relevant documents

## Difficulties in Evaluatin IR Systems

    - Effecctiveness related to relevancy of retrieved items
    - Relevancy not typically binary but continuous
    - Relevancy is binary, can be difficult judgment to make
    - Relevancy, from a human standpoint
      - Subjective
      - situational
      - cognitive
      - Dynamic

## Human Labeled Corpora (Gold Standard)

    - Start with corpus of documents
    - Collect a set of queries for this corpus
    - 1+ human experts exhaustively label the relevant documents for each query
    - Assume binary relevance judgments (typically)
    - Requries considereable human effort for large docuemnt/query corpora

## Precision and Recall

    recall = Number of relevant documents retrieved/total number of relevant documents
    precision = number of relevant documents retrieved/total number of documents retrieved

    Irrelevant: retrieved & irrelevant, not retrieved & irrelevant
    Relevatn:   retrieved & relelvant,  not retriieved BUT relevant
                retrieved                relevant
    recall gives what percent returned of relevant documents

## Determining Recall is Difficult

    - Precision vs recall
      - precision : the ability to retrieve top-ranked documents that are mostly relevant
      - recall : the ability of the search to find all of the relevant items in the corpus
    - Total number of relevant items is sometimes not avilable
      - sample across the database and perfrom relevance judgment on these items
    - Ideal: Precision 1, Recall 1
      - Precision 1: relevant documents but misses many useful
      - Recall 1: returns most relevant, but lots of junk

## Interpolating a Recall/Precision Curve

## Average Precision

    - AP
    - Average precisoin of each relevant document retrieved
    - Precision of unretrieved 0
    - Mean precision: over all queries
    - From example:

        AP = (1 + 1 + 0.75 + 0.667 + 0.38) / 5 =  0.7594

    - the higher the value, the better

## F-Measure

    - One mesure of performance that take  into account both recall and precision
    - van Rijbergen, 1979
    - Harmonic mean of recall and precision
    - Compared to arithmetic mean, both need to be high for harmonic mean to be high

    F = (2PR)/(P+R) = 2/(1/R + 1/P)

## NDCG

    - Normalized Discounted Cumulative Gain
    - Before, relvance had been binary
    - Popular measure for evaluating web search
    - want partially relevant
    - Two Assumptions
      - Highly relevant more useful than marginally relevant
      - The lower ranked, the less useful, so less likely to be examined
      1 d7
      2 d4 R = 1/10 x W1
      3 d3  w1 > w2
      4 d10 R = 2/10 x W2
      wr = 1/log(r)

## Discounted Cumulative Gain

    - Uses graded relevance as a measure of the usefulness, or gain, from examining a doument
    - Gain is accumulated starting at the top of the ranking and may be reduced, or discounted at lower ranks
    - Typical discount is 1/log(rank)
    - with base 2, the discount at rank 4 is 1/2 and at rank 8 it is 1/3

## DCG Example

    - 10 ranked documents judged on 0-3 relevance scale
      - 3,2,3,0,0,1,2,2,3,0
    - discounted gain
      - 3,2.1,3/1.59,0,0,1/2.59...
      - = 3,2,1.89,0,0...
    - DCG
      - 3,5,6.89,6.89,7.28...
    - Formula
      DCGp = rel1 + sum(reli/log2(i), i=2, p)
           = 3 + 2/log2(2) + 3/log2(3) + 2 + 3/1.59

      GCG: 3 3 3 2 2 1 0 0 0

    - Get a high quality search result, in order

## Normalized DCG

    - Perfect ranking
    - ideal DCG values
    - NDCG values (divide actual by ideal)
    - NDCG <= 1 at any rank position

## What is the right measure

    - Precision: I'm feeling lucky
    - Recall: maximizing coverage of topic
    - F: explore P/R tradeoff

## Books

    - Cracking the code interview
    - Data structures
      - Linked -> single, double
      - Binary
      - sorts -> merge, quick

END
