# COEN 169 Web Infromation Management 05_26_16

## Text Classification

  Vector Space

    - Representation: TF, TF.IDF, Okapi, Binary
    - Similarity: cosine, dot product, euclideant dist, etc.
    - Algorithm: Rochio Algorithm
        - K-nearest neighborhood (KNN)
            - very similar to collaborative filtering

  Statistical Approach

    - P(D|Q) = P(Q|D)P(D) / P(Q)
      - but P(Q) is the same for all
      - so only need P(Q|D)P(D)
        - query likelihood * document prior
        - Query likelihood: P(T_i|D) * P(t_n|D), t in Q
    - Spam vs Not Spam
      - classify into one of the two categories
      - compute P(C_1|D) vs P(C_2|D)
    - Category -> Document
    - Document -> query
    - P(C_1|D)P(C_1) / P(D) alpha P(D|C_1)P(C_1)
    - P(t|C_1) = tft_c_1 / N_C_1
      - need to do smoothing -> What if t doesn't appear?
      - Add one: P(t_C_1) = (tft_c_1 + 1) / (N_C_1 + |V|)
    - category prior
    - Naive Bayes

  Example

    - Classier | Ground Truth
      - Y | Y -> True Positive, TP
      - N | N -> True Negative, TN
      - Y | N -> False Positive, FP
      - N | Y -> False Negative, FN

    - Precision = # of TF / (TP + FP)
    - Recall = # of TP / (TP + FN)
    - F = (2PR)/(P+R)
    - Accuracy = (TP + TN) / (TP + TN + FP + FN)

     - rating for 1 * similarity for 1 and 3 + ...

## Text Clustering

    - supervised learning

## K-Means

    - Best known clustering
    - sort of like rochio
    - linear algorithm
      - find a linear boundary to separate the two categories
      - find centroid, learn boundary

    - k-mean
      - pretend you know the centroid for each category
      - draw boundaries based on the the centroids
      - later improve through iteration

    ```
          //initilize the centroids
          while (centroid not changed) {
            //recompute the membership of docs based on the centroid
            //recompute the centroids based on the current
            //based on the current assignments of the categories of docs
          }

    ```

## Document Representations in Clustering

    - Vector space model
    - As in vector space classification, we measure relatedness between vectors by
      - Euclidean distance
    - Which is almost equivalent to cosine similarity

## K-Mean

    - an iterative algorithm
    - Clusters based on centroids (akaa the mean) of points in a cluster
    - Reassignment
    - Recompute

## Optimality of K-means

    - Convergence does not mean that we converge to the optimal clustering
    - This is the weakness of K-means
    - If we start with a bad set of seeds, teh resulting clustering can be horrible
    - But k-mean doesn't always work
      - concentric circles won't cluster properly
      - spirals won't cluster properly either

END
